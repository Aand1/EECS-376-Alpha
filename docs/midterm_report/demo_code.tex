\part{Demo Code}

\section{Demo 1}
For demo 1 our robot had to navigate the hall using only dead
reckoning.  The code for demo 1 is attached in the src/demo1
directory.

Our code has 2 functions to help main.  We separated the code for turn
in place and the code for straight segments into two functions to make
it easier to schedule a path and update the code. The direction of the
turn and straight is determined by the sign of the input. A positive
will move the robot forward or counter clockwise and negative will
move the robot backwards or clockwise.

In the spatial velocity profiler there is also a State class which
keeps track of ideal state based on the integration of velocity commands

\section{Demo 2}
The source code for demo 2 is included in src/demo2.
To start we programmed a simple E-Stop publisher for the simulator.
This allowed us to test our E-Stop resume code in the simulator
first.  However, it didn't transfer perfectly as we found out there is
a short delay between disabling E-Stop and the motors responding to
commands.

To make this work we simply added a trap statement at the top of each
of our function's main loop.  Essentially it said that if the estop
was enabled sleep for a bit and then restart the loop and try again.
This froze velocity profiler's internal values until the E-Stop was disabled.


\section{Demo 3}
The source code for demo 3 is included in src/demo3. Demo 3 functions
almost identically to Demo 2 except an obstacles trap is added after
the estop trap.  So that the order of trap preference is E-Stops then
obstacles.

The first time and obstacle is detected the robots current position,
current velocity and the object distance are used to calculate a
constant deceleration rate.  This rate is then used in the second part
of the trap until the obstacle is removed or the robot is at a
complete halt.

\section{Demo 4}
The source code for demo 4 is included in src/demo4.  This is the most
major rewrite yet.

Velocity profiler now takes in path segments from Path Publisher.  It
uses the path segments to calculate the desired distances and angles
and then executes normally. It uses the velocity commands issued by
steering to integrate in the State class.

Also the work to split State into a separate class has started.  This
new class will only integrate contributions along the path and will be
separate from velocity profiler making it easier to upgrade.  State
will also be upgrade to integrate only along the path, which will
eliminate the error of integrating the steering corrections and ending
segments early.

Steering currently uses a linear steering algorithm although an
upgrade to the non-linear algorithm is planned.  It currently
saturates its corrections about velocity profiler's desired
velocities.  The way we are currently implementing it this adds a
little bit of a deadzone close to zero.  This will be smoothed out in
the future.

Path Publisher has 5 hard coded segments.  It continously sends the
latest segment until velocity profiler signals that it is done by
setting segComplete in segStatus to true.  At that point Path
Publisher switches to the next segment and the cycle continues until
all 5 paths have been completed.

\todo[color=green,inline]{Update Demo Descriptions}

\section{Obstacle Avoidance Demo}
The source code for the obstacle avoidance demo is included in src/obstacle\_avoidance

Because of our transition to python we were not able to devote as much time to developing the avoidance demo code. Putting this on top of our new algorithms for velocity profiler meant that a lot of bugs were introduced in the obstacle avoidance. Many of these were addressed when velocity profiler was rewritten for the final demo.

For this demo we are still publishing static waypoints as in the last demo. However, when an obstacle is detected by the LIDAR in the robots path, the robot will stop for 3 seconds. If the obstacle has not moved after 3 seconds the robot assumes the obstacle is not going to move. It then sends an abort signal to the path publisher. The path publisher then creates two arcs to move the robot over to the opposite side of the hallway.  The path publisher then starts publishing short straight segments until the LIDAR no longer detects an obstacle on its preferred side of the hallway. It will then publish two more arc segments which put the robot back on its original path.

\section{Kinect Demo 1}
The source code for the first Kinect demo is included in src/kinect\_demo1

For this demo we detect blobs of orange using RGB image data from the Kinect camera. We then find the centroid of the largest blob and publish its offset from the center of the camera. We then apply a proportionality constant to that number and use it to adjust the steering spin commands.

\section{Kinect  Demo 2}
The source code for the second Kinect demo is included in src/kinect\_demo2

For this demo we filter the image using HSV values and then using depth information for points in a certain z-range.  The points are then sorted into bins and the centroid of the closest bin containing points is used as the next point in the strap path.

The steering node simply steers to that point published. If no centroid is detected steering will spin in a circle until the strap is detected again. The robot will drive up an down the strap until its batteries run out or it is stopped.

A video of this demonstration is posted at \url{http://www.youtube.com/watch?v=iIT8uX7mIvI}

\section{Final}
The source code for the final demo is included in src/final

Unfortunately we were not able to finish this demo by the time it was due. This is mostly due to fighting with the costmap.

In this demo the plan was to run a goal planner that published major waypoints and switched modes from obstacle avoidance to magnet pickup.  The waypoints from the goal planner would be used by a path finding algorithm such as A* or brushfire. The points from these algorithms would then be converted to path segments by path planner. These path segments would be turned into a trajectory by velocity profiler and corrections would be applied by steering.

In order to help pick up the magnet we made a transform to the magnet\_frame which we obtained via tape measurements. The idea was when trying to line up with the metal the robot would check if the metal was at 0.0 in the magnet frame. If not it would apply corrections as necessary.
